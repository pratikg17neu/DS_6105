{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25ZsVi0wwd-r"
      },
      "source": [
        "\n",
        "\n",
        "### TMNIST (Typography MNIST) Glyphs: A database of over 500,000 MNIST style images made from 1,812 unique glyphs and 2,990 font-styles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbbLvymp7Eg4"
      },
      "outputs": [],
      "source": [
        "# Importing all the necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjOPbBg9-9nr"
      },
      "source": [
        "# Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eumOMNhR8ntS"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('Glyphs_TMNIST_updated.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTBtHKkfx4kf",
        "outputId": "8ee2d8d1-3a19-44bd-9e09-09af1b5c52a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100981 entries, 0 to 100980\n",
            "Columns: 787 entries, font_name to 784\n",
            "dtypes: float64(582), int64(202), object(3)\n",
            "memory usage: 606.3+ MB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cncdQFB_AQ_M"
      },
      "source": [
        "## Shape of the Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt3nd6wmmFKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcdfe4a-5bc9-45c8-a772-c8a76a2c2a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the Dataframe: (100981, 787)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Shape of the Dataframe: {data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if-TPowcGgzI"
      },
      "source": [
        "We will create the train and test set for the fonts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yow4VTu-oarX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2edb95af-e686-463b-fbcd-7a844ed83a6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Alphanumeric and Symbols List\n",
        "\n",
        "symbolsArr = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
        "           'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \n",
        "           '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
        "           '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@','[',']','\\\\','^','_','`','{','}',\"|\",'~']\n",
        "\n",
        "len(symbolsArr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFd4ZA1Ayr7i"
      },
      "source": [
        "## Get all the symbols present in the array from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJnpc6KdG556"
      },
      "outputs": [],
      "source": [
        "data = data[data.label.isin(symbolsArr)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wp44gTwyz65"
      },
      "source": [
        "### DataFrame Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWmaWMquHB0C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "5971df05-69ae-42a9-f4d7-eed733a494f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       count      mean       std  min  25%  50%  75%   max\n",
              "1    58105.0  0.000000  0.000000  0.0  0.0  0.0  0.0   0.0\n",
              "2    58105.0  0.000000  0.000000  0.0  0.0  0.0  0.0   0.0\n",
              "3    58105.0  0.000000  0.000000  0.0  0.0  0.0  0.0   0.0\n",
              "4    58105.0  0.000000  0.000000  0.0  0.0  0.0  0.0   0.0\n",
              "5    58105.0  0.000000  0.000000  0.0  0.0  0.0  0.0   0.0\n",
              "..       ...       ...       ...  ...  ...  ...  ...   ...\n",
              "780  58104.0  0.000447  0.084001  0.0  0.0  0.0  0.0  19.0\n",
              "781  58104.0  0.000327  0.078823  0.0  0.0  0.0  0.0  19.0\n",
              "782  58104.0  0.000000  0.000000  0.0  0.0  0.0  0.0   0.0\n",
              "783  58104.0  0.000000  0.000000  0.0  0.0  0.0  0.0   0.0\n",
              "784  58104.0  0.000000  0.000000  0.0  0.0  0.0  0.0   0.0\n",
              "\n",
              "[784 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3bced752-a5e1-4d19-ae7b-f1588068b805\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>58105.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>58105.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>58105.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>58105.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>58105.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>58104.0</td>\n",
              "      <td>0.000447</td>\n",
              "      <td>0.084001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>58104.0</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>0.078823</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782</th>\n",
              "      <td>58104.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>783</th>\n",
              "      <td>58104.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784</th>\n",
              "      <td>58104.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>784 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3bced752-a5e1-4d19-ae7b-f1588068b805')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3bced752-a5e1-4d19-ae7b-f1588068b805 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3bced752-a5e1-4d19-ae7b-f1588068b805');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "\n",
        "data.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vLSQasloyJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f5acdeb-73d1-4f07-f0a0-7f0fa810447a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 58105 entries, 0 to 100980\n",
            "Data columns (total 787 columns):\n",
            " #    Column      Dtype  \n",
            "---   ------      -----  \n",
            " 0    font_name   object \n",
            " 1    glyph_name  object \n",
            " 2    label       object \n",
            " 3    1           int64  \n",
            " 4    2           int64  \n",
            " 5    3           int64  \n",
            " 6    4           int64  \n",
            " 7    5           int64  \n",
            " 8    6           int64  \n",
            " 9    7           int64  \n",
            " 10   8           int64  \n",
            " 11   9           int64  \n",
            " 12   10          int64  \n",
            " 13   11          int64  \n",
            " 14   12          int64  \n",
            " 15   13          int64  \n",
            " 16   14          int64  \n",
            " 17   15          int64  \n",
            " 18   16          int64  \n",
            " 19   17          int64  \n",
            " 20   18          int64  \n",
            " 21   19          int64  \n",
            " 22   20          int64  \n",
            " 23   21          int64  \n",
            " 24   22          int64  \n",
            " 25   23          int64  \n",
            " 26   24          int64  \n",
            " 27   25          int64  \n",
            " 28   26          int64  \n",
            " 29   27          int64  \n",
            " 30   28          int64  \n",
            " 31   29          int64  \n",
            " 32   30          int64  \n",
            " 33   31          int64  \n",
            " 34   32          int64  \n",
            " 35   33          int64  \n",
            " 36   34          int64  \n",
            " 37   35          int64  \n",
            " 38   36          int64  \n",
            " 39   37          int64  \n",
            " 40   38          int64  \n",
            " 41   39          int64  \n",
            " 42   40          int64  \n",
            " 43   41          int64  \n",
            " 44   42          int64  \n",
            " 45   43          int64  \n",
            " 46   44          int64  \n",
            " 47   45          int64  \n",
            " 48   46          int64  \n",
            " 49   47          int64  \n",
            " 50   48          int64  \n",
            " 51   49          int64  \n",
            " 52   50          int64  \n",
            " 53   51          int64  \n",
            " 54   52          int64  \n",
            " 55   53          int64  \n",
            " 56   54          int64  \n",
            " 57   55          int64  \n",
            " 58   56          int64  \n",
            " 59   57          int64  \n",
            " 60   58          int64  \n",
            " 61   59          int64  \n",
            " 62   60          int64  \n",
            " 63   61          int64  \n",
            " 64   62          int64  \n",
            " 65   63          int64  \n",
            " 66   64          int64  \n",
            " 67   65          int64  \n",
            " 68   66          int64  \n",
            " 69   67          int64  \n",
            " 70   68          int64  \n",
            " 71   69          int64  \n",
            " 72   70          int64  \n",
            " 73   71          int64  \n",
            " 74   72          int64  \n",
            " 75   73          int64  \n",
            " 76   74          int64  \n",
            " 77   75          int64  \n",
            " 78   76          int64  \n",
            " 79   77          int64  \n",
            " 80   78          int64  \n",
            " 81   79          int64  \n",
            " 82   80          int64  \n",
            " 83   81          int64  \n",
            " 84   82          int64  \n",
            " 85   83          int64  \n",
            " 86   84          int64  \n",
            " 87   85          int64  \n",
            " 88   86          int64  \n",
            " 89   87          int64  \n",
            " 90   88          int64  \n",
            " 91   89          int64  \n",
            " 92   90          int64  \n",
            " 93   91          int64  \n",
            " 94   92          int64  \n",
            " 95   93          int64  \n",
            " 96   94          int64  \n",
            " 97   95          int64  \n",
            " 98   96          int64  \n",
            " 99   97          int64  \n",
            " 100  98          int64  \n",
            " 101  99          int64  \n",
            " 102  100         int64  \n",
            " 103  101         int64  \n",
            " 104  102         int64  \n",
            " 105  103         int64  \n",
            " 106  104         int64  \n",
            " 107  105         int64  \n",
            " 108  106         int64  \n",
            " 109  107         int64  \n",
            " 110  108         int64  \n",
            " 111  109         int64  \n",
            " 112  110         int64  \n",
            " 113  111         int64  \n",
            " 114  112         int64  \n",
            " 115  113         int64  \n",
            " 116  114         int64  \n",
            " 117  115         int64  \n",
            " 118  116         int64  \n",
            " 119  117         int64  \n",
            " 120  118         int64  \n",
            " 121  119         int64  \n",
            " 122  120         int64  \n",
            " 123  121         int64  \n",
            " 124  122         int64  \n",
            " 125  123         int64  \n",
            " 126  124         int64  \n",
            " 127  125         int64  \n",
            " 128  126         int64  \n",
            " 129  127         int64  \n",
            " 130  128         int64  \n",
            " 131  129         int64  \n",
            " 132  130         int64  \n",
            " 133  131         int64  \n",
            " 134  132         int64  \n",
            " 135  133         int64  \n",
            " 136  134         int64  \n",
            " 137  135         int64  \n",
            " 138  136         int64  \n",
            " 139  137         int64  \n",
            " 140  138         int64  \n",
            " 141  139         int64  \n",
            " 142  140         int64  \n",
            " 143  141         int64  \n",
            " 144  142         int64  \n",
            " 145  143         int64  \n",
            " 146  144         int64  \n",
            " 147  145         int64  \n",
            " 148  146         int64  \n",
            " 149  147         int64  \n",
            " 150  148         int64  \n",
            " 151  149         int64  \n",
            " 152  150         int64  \n",
            " 153  151         int64  \n",
            " 154  152         int64  \n",
            " 155  153         int64  \n",
            " 156  154         int64  \n",
            " 157  155         int64  \n",
            " 158  156         int64  \n",
            " 159  157         int64  \n",
            " 160  158         int64  \n",
            " 161  159         int64  \n",
            " 162  160         int64  \n",
            " 163  161         int64  \n",
            " 164  162         int64  \n",
            " 165  163         int64  \n",
            " 166  164         int64  \n",
            " 167  165         int64  \n",
            " 168  166         int64  \n",
            " 169  167         int64  \n",
            " 170  168         int64  \n",
            " 171  169         int64  \n",
            " 172  170         int64  \n",
            " 173  171         int64  \n",
            " 174  172         int64  \n",
            " 175  173         int64  \n",
            " 176  174         int64  \n",
            " 177  175         int64  \n",
            " 178  176         int64  \n",
            " 179  177         int64  \n",
            " 180  178         int64  \n",
            " 181  179         int64  \n",
            " 182  180         int64  \n",
            " 183  181         int64  \n",
            " 184  182         int64  \n",
            " 185  183         int64  \n",
            " 186  184         int64  \n",
            " 187  185         int64  \n",
            " 188  186         int64  \n",
            " 189  187         int64  \n",
            " 190  188         int64  \n",
            " 191  189         int64  \n",
            " 192  190         int64  \n",
            " 193  191         int64  \n",
            " 194  192         int64  \n",
            " 195  193         int64  \n",
            " 196  194         int64  \n",
            " 197  195         int64  \n",
            " 198  196         int64  \n",
            " 199  197         int64  \n",
            " 200  198         int64  \n",
            " 201  199         int64  \n",
            " 202  200         int64  \n",
            " 203  201         int64  \n",
            " 204  202         int64  \n",
            " 205  203         float64\n",
            " 206  204         float64\n",
            " 207  205         float64\n",
            " 208  206         float64\n",
            " 209  207         float64\n",
            " 210  208         float64\n",
            " 211  209         float64\n",
            " 212  210         float64\n",
            " 213  211         float64\n",
            " 214  212         float64\n",
            " 215  213         float64\n",
            " 216  214         float64\n",
            " 217  215         float64\n",
            " 218  216         float64\n",
            " 219  217         float64\n",
            " 220  218         float64\n",
            " 221  219         float64\n",
            " 222  220         float64\n",
            " 223  221         float64\n",
            " 224  222         float64\n",
            " 225  223         float64\n",
            " 226  224         float64\n",
            " 227  225         float64\n",
            " 228  226         float64\n",
            " 229  227         float64\n",
            " 230  228         float64\n",
            " 231  229         float64\n",
            " 232  230         float64\n",
            " 233  231         float64\n",
            " 234  232         float64\n",
            " 235  233         float64\n",
            " 236  234         float64\n",
            " 237  235         float64\n",
            " 238  236         float64\n",
            " 239  237         float64\n",
            " 240  238         float64\n",
            " 241  239         float64\n",
            " 242  240         float64\n",
            " 243  241         float64\n",
            " 244  242         float64\n",
            " 245  243         float64\n",
            " 246  244         float64\n",
            " 247  245         float64\n",
            " 248  246         float64\n",
            " 249  247         float64\n",
            " 250  248         float64\n",
            " 251  249         float64\n",
            " 252  250         float64\n",
            " 253  251         float64\n",
            " 254  252         float64\n",
            " 255  253         float64\n",
            " 256  254         float64\n",
            " 257  255         float64\n",
            " 258  256         float64\n",
            " 259  257         float64\n",
            " 260  258         float64\n",
            " 261  259         float64\n",
            " 262  260         float64\n",
            " 263  261         float64\n",
            " 264  262         float64\n",
            " 265  263         float64\n",
            " 266  264         float64\n",
            " 267  265         float64\n",
            " 268  266         float64\n",
            " 269  267         float64\n",
            " 270  268         float64\n",
            " 271  269         float64\n",
            " 272  270         float64\n",
            " 273  271         float64\n",
            " 274  272         float64\n",
            " 275  273         float64\n",
            " 276  274         float64\n",
            " 277  275         float64\n",
            " 278  276         float64\n",
            " 279  277         float64\n",
            " 280  278         float64\n",
            " 281  279         float64\n",
            " 282  280         float64\n",
            " 283  281         float64\n",
            " 284  282         float64\n",
            " 285  283         float64\n",
            " 286  284         float64\n",
            " 287  285         float64\n",
            " 288  286         float64\n",
            " 289  287         float64\n",
            " 290  288         float64\n",
            " 291  289         float64\n",
            " 292  290         float64\n",
            " 293  291         float64\n",
            " 294  292         float64\n",
            " 295  293         float64\n",
            " 296  294         float64\n",
            " 297  295         float64\n",
            " 298  296         float64\n",
            " 299  297         float64\n",
            " 300  298         float64\n",
            " 301  299         float64\n",
            " 302  300         float64\n",
            " 303  301         float64\n",
            " 304  302         float64\n",
            " 305  303         float64\n",
            " 306  304         float64\n",
            " 307  305         float64\n",
            " 308  306         float64\n",
            " 309  307         float64\n",
            " 310  308         float64\n",
            " 311  309         float64\n",
            " 312  310         float64\n",
            " 313  311         float64\n",
            " 314  312         float64\n",
            " 315  313         float64\n",
            " 316  314         float64\n",
            " 317  315         float64\n",
            " 318  316         float64\n",
            " 319  317         float64\n",
            " 320  318         float64\n",
            " 321  319         float64\n",
            " 322  320         float64\n",
            " 323  321         float64\n",
            " 324  322         float64\n",
            " 325  323         float64\n",
            " 326  324         float64\n",
            " 327  325         float64\n",
            " 328  326         float64\n",
            " 329  327         float64\n",
            " 330  328         float64\n",
            " 331  329         float64\n",
            " 332  330         float64\n",
            " 333  331         float64\n",
            " 334  332         float64\n",
            " 335  333         float64\n",
            " 336  334         float64\n",
            " 337  335         float64\n",
            " 338  336         float64\n",
            " 339  337         float64\n",
            " 340  338         float64\n",
            " 341  339         float64\n",
            " 342  340         float64\n",
            " 343  341         float64\n",
            " 344  342         float64\n",
            " 345  343         float64\n",
            " 346  344         float64\n",
            " 347  345         float64\n",
            " 348  346         float64\n",
            " 349  347         float64\n",
            " 350  348         float64\n",
            " 351  349         float64\n",
            " 352  350         float64\n",
            " 353  351         float64\n",
            " 354  352         float64\n",
            " 355  353         float64\n",
            " 356  354         float64\n",
            " 357  355         float64\n",
            " 358  356         float64\n",
            " 359  357         float64\n",
            " 360  358         float64\n",
            " 361  359         float64\n",
            " 362  360         float64\n",
            " 363  361         float64\n",
            " 364  362         float64\n",
            " 365  363         float64\n",
            " 366  364         float64\n",
            " 367  365         float64\n",
            " 368  366         float64\n",
            " 369  367         float64\n",
            " 370  368         float64\n",
            " 371  369         float64\n",
            " 372  370         float64\n",
            " 373  371         float64\n",
            " 374  372         float64\n",
            " 375  373         float64\n",
            " 376  374         float64\n",
            " 377  375         float64\n",
            " 378  376         float64\n",
            " 379  377         float64\n",
            " 380  378         float64\n",
            " 381  379         float64\n",
            " 382  380         float64\n",
            " 383  381         float64\n",
            " 384  382         float64\n",
            " 385  383         float64\n",
            " 386  384         float64\n",
            " 387  385         float64\n",
            " 388  386         float64\n",
            " 389  387         float64\n",
            " 390  388         float64\n",
            " 391  389         float64\n",
            " 392  390         float64\n",
            " 393  391         float64\n",
            " 394  392         float64\n",
            " 395  393         float64\n",
            " 396  394         float64\n",
            " 397  395         float64\n",
            " 398  396         float64\n",
            " 399  397         float64\n",
            " 400  398         float64\n",
            " 401  399         float64\n",
            " 402  400         float64\n",
            " 403  401         float64\n",
            " 404  402         float64\n",
            " 405  403         float64\n",
            " 406  404         float64\n",
            " 407  405         float64\n",
            " 408  406         float64\n",
            " 409  407         float64\n",
            " 410  408         float64\n",
            " 411  409         float64\n",
            " 412  410         float64\n",
            " 413  411         float64\n",
            " 414  412         float64\n",
            " 415  413         float64\n",
            " 416  414         float64\n",
            " 417  415         float64\n",
            " 418  416         float64\n",
            " 419  417         float64\n",
            " 420  418         float64\n",
            " 421  419         float64\n",
            " 422  420         float64\n",
            " 423  421         float64\n",
            " 424  422         float64\n",
            " 425  423         float64\n",
            " 426  424         float64\n",
            " 427  425         float64\n",
            " 428  426         float64\n",
            " 429  427         float64\n",
            " 430  428         float64\n",
            " 431  429         float64\n",
            " 432  430         float64\n",
            " 433  431         float64\n",
            " 434  432         float64\n",
            " 435  433         float64\n",
            " 436  434         float64\n",
            " 437  435         float64\n",
            " 438  436         float64\n",
            " 439  437         float64\n",
            " 440  438         float64\n",
            " 441  439         float64\n",
            " 442  440         float64\n",
            " 443  441         float64\n",
            " 444  442         float64\n",
            " 445  443         float64\n",
            " 446  444         float64\n",
            " 447  445         float64\n",
            " 448  446         float64\n",
            " 449  447         float64\n",
            " 450  448         float64\n",
            " 451  449         float64\n",
            " 452  450         float64\n",
            " 453  451         float64\n",
            " 454  452         float64\n",
            " 455  453         float64\n",
            " 456  454         float64\n",
            " 457  455         float64\n",
            " 458  456         float64\n",
            " 459  457         float64\n",
            " 460  458         float64\n",
            " 461  459         float64\n",
            " 462  460         float64\n",
            " 463  461         float64\n",
            " 464  462         float64\n",
            " 465  463         float64\n",
            " 466  464         float64\n",
            " 467  465         float64\n",
            " 468  466         float64\n",
            " 469  467         float64\n",
            " 470  468         float64\n",
            " 471  469         float64\n",
            " 472  470         float64\n",
            " 473  471         float64\n",
            " 474  472         float64\n",
            " 475  473         float64\n",
            " 476  474         float64\n",
            " 477  475         float64\n",
            " 478  476         float64\n",
            " 479  477         float64\n",
            " 480  478         float64\n",
            " 481  479         float64\n",
            " 482  480         float64\n",
            " 483  481         float64\n",
            " 484  482         float64\n",
            " 485  483         float64\n",
            " 486  484         float64\n",
            " 487  485         float64\n",
            " 488  486         float64\n",
            " 489  487         float64\n",
            " 490  488         float64\n",
            " 491  489         float64\n",
            " 492  490         float64\n",
            " 493  491         float64\n",
            " 494  492         float64\n",
            " 495  493         float64\n",
            " 496  494         float64\n",
            " 497  495         float64\n",
            " 498  496         float64\n",
            " 499  497         float64\n",
            " 500  498         float64\n",
            " 501  499         float64\n",
            " 502  500         float64\n",
            " 503  501         float64\n",
            " 504  502         float64\n",
            " 505  503         float64\n",
            " 506  504         float64\n",
            " 507  505         float64\n",
            " 508  506         float64\n",
            " 509  507         float64\n",
            " 510  508         float64\n",
            " 511  509         float64\n",
            " 512  510         float64\n",
            " 513  511         float64\n",
            " 514  512         float64\n",
            " 515  513         float64\n",
            " 516  514         float64\n",
            " 517  515         float64\n",
            " 518  516         float64\n",
            " 519  517         float64\n",
            " 520  518         float64\n",
            " 521  519         float64\n",
            " 522  520         float64\n",
            " 523  521         float64\n",
            " 524  522         float64\n",
            " 525  523         float64\n",
            " 526  524         float64\n",
            " 527  525         float64\n",
            " 528  526         float64\n",
            " 529  527         float64\n",
            " 530  528         float64\n",
            " 531  529         float64\n",
            " 532  530         float64\n",
            " 533  531         float64\n",
            " 534  532         float64\n",
            " 535  533         float64\n",
            " 536  534         float64\n",
            " 537  535         float64\n",
            " 538  536         float64\n",
            " 539  537         float64\n",
            " 540  538         float64\n",
            " 541  539         float64\n",
            " 542  540         float64\n",
            " 543  541         float64\n",
            " 544  542         float64\n",
            " 545  543         float64\n",
            " 546  544         float64\n",
            " 547  545         float64\n",
            " 548  546         float64\n",
            " 549  547         float64\n",
            " 550  548         float64\n",
            " 551  549         float64\n",
            " 552  550         float64\n",
            " 553  551         float64\n",
            " 554  552         float64\n",
            " 555  553         float64\n",
            " 556  554         float64\n",
            " 557  555         float64\n",
            " 558  556         float64\n",
            " 559  557         float64\n",
            " 560  558         float64\n",
            " 561  559         float64\n",
            " 562  560         float64\n",
            " 563  561         float64\n",
            " 564  562         float64\n",
            " 565  563         float64\n",
            " 566  564         float64\n",
            " 567  565         float64\n",
            " 568  566         float64\n",
            " 569  567         float64\n",
            " 570  568         float64\n",
            " 571  569         float64\n",
            " 572  570         float64\n",
            " 573  571         float64\n",
            " 574  572         float64\n",
            " 575  573         float64\n",
            " 576  574         float64\n",
            " 577  575         float64\n",
            " 578  576         float64\n",
            " 579  577         float64\n",
            " 580  578         float64\n",
            " 581  579         float64\n",
            " 582  580         float64\n",
            " 583  581         float64\n",
            " 584  582         float64\n",
            " 585  583         float64\n",
            " 586  584         float64\n",
            " 587  585         float64\n",
            " 588  586         float64\n",
            " 589  587         float64\n",
            " 590  588         float64\n",
            " 591  589         float64\n",
            " 592  590         float64\n",
            " 593  591         float64\n",
            " 594  592         float64\n",
            " 595  593         float64\n",
            " 596  594         float64\n",
            " 597  595         float64\n",
            " 598  596         float64\n",
            " 599  597         float64\n",
            " 600  598         float64\n",
            " 601  599         float64\n",
            " 602  600         float64\n",
            " 603  601         float64\n",
            " 604  602         float64\n",
            " 605  603         float64\n",
            " 606  604         float64\n",
            " 607  605         float64\n",
            " 608  606         float64\n",
            " 609  607         float64\n",
            " 610  608         float64\n",
            " 611  609         float64\n",
            " 612  610         float64\n",
            " 613  611         float64\n",
            " 614  612         float64\n",
            " 615  613         float64\n",
            " 616  614         float64\n",
            " 617  615         float64\n",
            " 618  616         float64\n",
            " 619  617         float64\n",
            " 620  618         float64\n",
            " 621  619         float64\n",
            " 622  620         float64\n",
            " 623  621         float64\n",
            " 624  622         float64\n",
            " 625  623         float64\n",
            " 626  624         float64\n",
            " 627  625         float64\n",
            " 628  626         float64\n",
            " 629  627         float64\n",
            " 630  628         float64\n",
            " 631  629         float64\n",
            " 632  630         float64\n",
            " 633  631         float64\n",
            " 634  632         float64\n",
            " 635  633         float64\n",
            " 636  634         float64\n",
            " 637  635         float64\n",
            " 638  636         float64\n",
            " 639  637         float64\n",
            " 640  638         float64\n",
            " 641  639         float64\n",
            " 642  640         float64\n",
            " 643  641         float64\n",
            " 644  642         float64\n",
            " 645  643         float64\n",
            " 646  644         float64\n",
            " 647  645         float64\n",
            " 648  646         float64\n",
            " 649  647         float64\n",
            " 650  648         float64\n",
            " 651  649         float64\n",
            " 652  650         float64\n",
            " 653  651         float64\n",
            " 654  652         float64\n",
            " 655  653         float64\n",
            " 656  654         float64\n",
            " 657  655         float64\n",
            " 658  656         float64\n",
            " 659  657         float64\n",
            " 660  658         float64\n",
            " 661  659         float64\n",
            " 662  660         float64\n",
            " 663  661         float64\n",
            " 664  662         float64\n",
            " 665  663         float64\n",
            " 666  664         float64\n",
            " 667  665         float64\n",
            " 668  666         float64\n",
            " 669  667         float64\n",
            " 670  668         float64\n",
            " 671  669         float64\n",
            " 672  670         float64\n",
            " 673  671         float64\n",
            " 674  672         float64\n",
            " 675  673         float64\n",
            " 676  674         float64\n",
            " 677  675         float64\n",
            " 678  676         float64\n",
            " 679  677         float64\n",
            " 680  678         float64\n",
            " 681  679         float64\n",
            " 682  680         float64\n",
            " 683  681         float64\n",
            " 684  682         float64\n",
            " 685  683         float64\n",
            " 686  684         float64\n",
            " 687  685         float64\n",
            " 688  686         float64\n",
            " 689  687         float64\n",
            " 690  688         float64\n",
            " 691  689         float64\n",
            " 692  690         float64\n",
            " 693  691         float64\n",
            " 694  692         float64\n",
            " 695  693         float64\n",
            " 696  694         float64\n",
            " 697  695         float64\n",
            " 698  696         float64\n",
            " 699  697         float64\n",
            " 700  698         float64\n",
            " 701  699         float64\n",
            " 702  700         float64\n",
            " 703  701         float64\n",
            " 704  702         float64\n",
            " 705  703         float64\n",
            " 706  704         float64\n",
            " 707  705         float64\n",
            " 708  706         float64\n",
            " 709  707         float64\n",
            " 710  708         float64\n",
            " 711  709         float64\n",
            " 712  710         float64\n",
            " 713  711         float64\n",
            " 714  712         float64\n",
            " 715  713         float64\n",
            " 716  714         float64\n",
            " 717  715         float64\n",
            " 718  716         float64\n",
            " 719  717         float64\n",
            " 720  718         float64\n",
            " 721  719         float64\n",
            " 722  720         float64\n",
            " 723  721         float64\n",
            " 724  722         float64\n",
            " 725  723         float64\n",
            " 726  724         float64\n",
            " 727  725         float64\n",
            " 728  726         float64\n",
            " 729  727         float64\n",
            " 730  728         float64\n",
            " 731  729         float64\n",
            " 732  730         float64\n",
            " 733  731         float64\n",
            " 734  732         float64\n",
            " 735  733         float64\n",
            " 736  734         float64\n",
            " 737  735         float64\n",
            " 738  736         float64\n",
            " 739  737         float64\n",
            " 740  738         float64\n",
            " 741  739         float64\n",
            " 742  740         float64\n",
            " 743  741         float64\n",
            " 744  742         float64\n",
            " 745  743         float64\n",
            " 746  744         float64\n",
            " 747  745         float64\n",
            " 748  746         float64\n",
            " 749  747         float64\n",
            " 750  748         float64\n",
            " 751  749         float64\n",
            " 752  750         float64\n",
            " 753  751         float64\n",
            " 754  752         float64\n",
            " 755  753         float64\n",
            " 756  754         float64\n",
            " 757  755         float64\n",
            " 758  756         float64\n",
            " 759  757         float64\n",
            " 760  758         float64\n",
            " 761  759         float64\n",
            " 762  760         float64\n",
            " 763  761         float64\n",
            " 764  762         float64\n",
            " 765  763         float64\n",
            " 766  764         float64\n",
            " 767  765         float64\n",
            " 768  766         float64\n",
            " 769  767         float64\n",
            " 770  768         float64\n",
            " 771  769         float64\n",
            " 772  770         float64\n",
            " 773  771         float64\n",
            " 774  772         float64\n",
            " 775  773         float64\n",
            " 776  774         float64\n",
            " 777  775         float64\n",
            " 778  776         float64\n",
            " 779  777         float64\n",
            " 780  778         float64\n",
            " 781  779         float64\n",
            " 782  780         float64\n",
            " 783  781         float64\n",
            " 784  782         float64\n",
            " 785  783         float64\n",
            " 786  784         float64\n",
            "dtypes: float64(582), int64(202), object(3)\n",
            "memory usage: 349.3+ MB\n"
          ]
        }
      ],
      "source": [
        "# DataFrame feature's Datatype\n",
        "\n",
        "data.info(verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycML_C4cAkbj"
      },
      "source": [
        "## Number of unique fonts in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J-q3AAXo2Zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7251055b-8666-4275-e0db-1f4a9bb56b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique fonts present in the Dataset: 680\n"
          ]
        }
      ],
      "source": [
        "print(f\"Unique fonts present in the Dataset: {len(data.font_name.unique())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ17nB82At93"
      },
      "source": [
        "## Unique character in the Dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjxQ_nWBo-qM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e86296e7-3c9e-46be-f389-1530b4cd9f8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique character present in the Dataset: 87\n"
          ]
        }
      ],
      "source": [
        "print(f\"Unique character present in the Dataset: {len(data.label.unique())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_kawZPApCH0"
      },
      "outputs": [],
      "source": [
        "# Spliting the Labels and the features\n",
        "X = data.drop(columns=['font_name','glyph_name','label']).values\n",
        "y = data[['label']].values\n",
        "del data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw9-Er05pEbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90baba97-26b1-4003-c3ba-fc4f1d32df2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X = X.astype('u1')\n",
        "X.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxtgg86JpGJu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "9f27e955-4c93-482e-c77d-462381529e91"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d11deba7275e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We will check the shape of x and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "# We will check the shape of x and y\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8T7D116pH54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "36649760-d57a-49fd-d427-14d17faf1e22"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x648 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Display few of the characters\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_images = X.reshape(-1,28,28)\n",
        "fig,axs = plt.subplots(3,3,figsize=(9,9))\n",
        "for i in range(9):\n",
        "    r=i//3\n",
        "    c=i%3\n",
        "    axs[r][c].set_xticks([])\n",
        "    axs[r][c].set_yticks([])\n",
        "    axs[r][c].imshow(X_images[i])\n",
        "plt.show()\n",
        "del X_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJlPgDIQpM0H"
      },
      "source": [
        "Split the Dataframe into Training and Test Dataframe¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHpHB1R_pNwI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "y_train = y_train.reshape((-1,))\n",
        "y_test = y_test.reshape((-1,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGMUNjs5pPZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d7016e-a81e-48b0-d3dc-19c7f6c0acfe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((52294, 784), (5811, 784))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkK-9HnfpRHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48fb3a21-3b11-4e9e-f4b2-75b90dff0ecb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((52294,), (5811,))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM1hpMjpqSRE"
      },
      "source": [
        "# Binarize labels¶\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTZRWI7iqVpz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06998ca1-9f67-4011-f0b8-cf94116f0338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train labels dimension:\n",
            "(52294,)\n",
            "Test labels dimension:\n",
            "(5811,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y_train_label = lb.fit_transform(y_train)\n",
        "y_test_label = lb.transform(y_test)\n",
        "print('Train labels dimension:');print(y_train.shape)\n",
        "print('Test labels dimension:');print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhN7GQEJqd2t"
      },
      "source": [
        "# Normalize the Training and Testing Dataset¶\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uREfDkNQqgDM"
      },
      "outputs": [],
      "source": [
        "# Normalizing the Dataset for the Neural Network\n",
        "\n",
        "X_train, X_test = np.true_divide(X_train, 255), np.true_divide(X_test, 255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRPUFUFGqpAe"
      },
      "source": [
        "# Dense Neural Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# we will use the dense neural network to run the model and test the accuracy of the training datasets"
      ],
      "metadata": {
        "id": "EVLJehT9Tn5F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z8WyNR9qwt1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEsoYSnMquRd"
      },
      "outputs": [],
      "source": [
        "# Create Model\n",
        "model = Sequential()\n",
        "model.add(Dense(250, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "model.add(Dense(125, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(y_train_label.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will run a sequesntial model with first 250 neurons as activation= relu, and then next 125 neurons as relu and then again we will add another \n",
        "layer of 100 neurons with activation as relu"
      ],
      "metadata": {
        "id": "HTwkHM_OUJ53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I59A8ZHZq0EF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce5a6b19-ef9e-4a60-eb2e-942ab69c029f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 250)               196250    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 125)               31375     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               12600     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 87)                8787      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 249,012\n",
            "Trainable params: 249,012\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Model Summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0A98wYlq4Uy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ef0d26-e838-4474-8824-1fc41cd37f73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "314/314 [==============================] - 3s 8ms/step - loss: 1.4242 - accuracy: 0.6505 - val_loss: 0.8050 - val_accuracy: 0.7805\n",
            "Epoch 2/2\n",
            "314/314 [==============================] - 2s 7ms/step - loss: 0.6558 - accuracy: 0.8169 - val_loss: 0.5904 - val_accuracy: 0.8314\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6d816b7090>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Configure the model and start training\n",
        "model.fit(X_train, y_train_label, epochs=2, batch_size=150, verbose=1, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After fitting the the model we can see that the accuracy is only 83% without training the dataset. Let's check the accuracy after training the datasets."
      ],
      "metadata": {
        "id": "9wT1zqRDUQkB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3mpHfc7tFdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73103126-602b-4d6f-aa2c-5a99ac8dc77f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "182/182 [==============================] - 0s 2ms/step - loss: 0.6076 - accuracy: 0.8288\n",
            "Test results - Accuracy: 0.828773021697998%\n"
          ]
        }
      ],
      "source": [
        "# Test the model after training\n",
        "test_results = model.evaluate(X_test, y_test_label, verbose=1)\n",
        "print(f'Test results - Accuracy: {test_results[1]}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGUScaaVtOJH"
      },
      "source": [
        "# Convolution Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8_-931EtRPo"
      },
      "outputs": [],
      "source": [
        "# Reshape X_train and X_test for CNN\n",
        "\n",
        "X_train = X_train.reshape(-1,28,28,1).astype('float32')\n",
        "X_test = X_test.reshape(-1,28,28,1).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufcic-3YtXLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a6df4f-0f6d-4841-c6ba-1230088f4cf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52294, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg7YfrT2s1jb"
      },
      "outputs": [],
      "source": [
        "#CNN Model\n",
        "\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(Conv2D(32,(4,4),input_shape = (28,28,1),activation = 'relu'))\n",
        "cnnmodel.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnnmodel.add(Conv2D(64,(3,3),activation = 'relu'))\n",
        "cnnmodel.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnnmodel.add(Dropout(0.2))\n",
        "cnnmodel.add(Flatten())\n",
        "cnnmodel.add(Dense(128,activation='relu'))\n",
        "cnnmodel.add(Dense(y_train_label.shape[1], activation='softmax'))\n",
        "cnnmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have used a cnn model to tain the data and shape the data using activation softmax. Afterwards we have compiled the model using categorical_crossentropy and optimizer as adam."
      ],
      "metadata": {
        "id": "sPD4Uby5UqVi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvKdHDkltar3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f053c016-44a5-495d-9e93-fa21411762b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 25, 25, 32)        544       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 12, 12, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5, 5, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               204928    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 87)                11223     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 235,191\n",
            "Trainable params: 235,191\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# CNN Model Summary\n",
        "\n",
        "cnnmodel.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXr8jB_ntfnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0778a19f-82ff-4d8f-afad-39de43f9d928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "471/471 [==============================] - 36s 75ms/step - loss: 1.1738 - accuracy: 0.7007 - val_loss: 0.5335 - val_accuracy: 0.8426\n",
            "Epoch 2/20\n",
            "471/471 [==============================] - 33s 70ms/step - loss: 0.4977 - accuracy: 0.8477 - val_loss: 0.4366 - val_accuracy: 0.8621\n",
            "Epoch 3/20\n",
            "471/471 [==============================] - 34s 73ms/step - loss: 0.3946 - accuracy: 0.8733 - val_loss: 0.3634 - val_accuracy: 0.8876\n",
            "Epoch 4/20\n",
            "471/471 [==============================] - 33s 70ms/step - loss: 0.3297 - accuracy: 0.8884 - val_loss: 0.3139 - val_accuracy: 0.9017\n",
            "Epoch 5/20\n",
            "234/471 [=============>................] - ETA: 16s - loss: 0.2879 - accuracy: 0.9006"
          ]
        }
      ],
      "source": [
        "# Train the CNN Model\n",
        "result = cnnmodel.fit(X_train, y_train_label, validation_split=0.1, epochs=20, batch_size=100, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aONw0ZrYtdhN"
      },
      "outputs": [],
      "source": [
        "# Test the CNN model after training\n",
        "test_results = cnnmodel.evaluate(X_test, y_test_label, verbose=1)\n",
        "print(f'Test results - Accuracy: {test_results[1]}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training anf running the cnn model we get an accuracy of 94.97%"
      ],
      "metadata": {
        "id": "YzbuHQHRVjTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion:\n",
        "\n",
        "We have an accuracy of 94.9% when we run on 20 epochs. It is clear that after using CNN model and training the datasets the accuracy increased drastically."
      ],
      "metadata": {
        "id": "wJE2SH_2Rd_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Licensing Copyright 2022 Pratik Gawand\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
        "\n",
        "Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
      ],
      "metadata": {
        "id": "pMbxtD1NRm_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HPO8TtvqRjdX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Neural_Network_Type_Classification_TMNIST_Glyphs_Pratik_Gawand_002102812.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}